{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index-readers-file in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (0.1.8)\n",
      "Requirement already satisfied: llama-index-vector-stores-postgres in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (0.1.2)\n",
      "Requirement already satisfied: llama-index-embeddings-huggingface in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (0.1.4)\n",
      "Requirement already satisfied: llama-index-llms-llama-cpp in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (0.1.3)\n",
      "Requirement already satisfied: llama-cpp-python in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (0.2.55)\n",
      "Requirement already satisfied: psycopg2-binary in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (2.9.9)\n",
      "Requirement already satisfied: pgvector in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (0.2.5)\n",
      "Requirement already satisfied: asyncpg in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (0.29.0)\n",
      "Requirement already satisfied: greenlet in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 10)) (3.0.1)\n",
      "Requirement already satisfied: sqlalchemy[asyncio] in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 9)) (2.0.25)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /opt/conda/lib/python3.11/site-packages (from llama-index-readers-file->-r requirements.txt (line 1)) (4.12.3)\n",
      "Requirement already satisfied: bs4<0.0.3,>=0.0.2 in /opt/conda/lib/python3.11/site-packages (from llama-index-readers-file->-r requirements.txt (line 1)) (0.0.2)\n",
      "Requirement already satisfied: llama-index-core<0.11.0,>=0.10.1 in /opt/conda/lib/python3.11/site-packages (from llama-index-readers-file->-r requirements.txt (line 1)) (0.10.18.post1)\n",
      "Requirement already satisfied: pymupdf<2.0.0,>=1.23.21 in /opt/conda/lib/python3.11/site-packages (from llama-index-readers-file->-r requirements.txt (line 1)) (1.23.26)\n",
      "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /opt/conda/lib/python3.11/site-packages (from llama-index-readers-file->-r requirements.txt (line 1)) (4.1.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r requirements.txt (line 3)) (0.21.3)\n",
      "Requirement already satisfied: torch<3.0.0,>=2.1.2 in /opt/conda/lib/python3.11/site-packages (from llama-index-embeddings-huggingface->-r requirements.txt (line 3)) (2.2.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.37.0 in /opt/conda/lib/python3.11/site-packages (from llama-index-embeddings-huggingface->-r requirements.txt (line 3)) (4.38.2)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.11/site-packages (from llama-cpp-python->-r requirements.txt (line 5)) (4.9.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /opt/conda/lib/python3.11/site-packages (from llama-cpp-python->-r requirements.txt (line 5)) (1.25.2)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /opt/conda/lib/python3.11/site-packages (from llama-cpp-python->-r requirements.txt (line 5)) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /opt/conda/lib/python3.11/site-packages (from llama-cpp-python->-r requirements.txt (line 5)) (3.1.3)\n",
      "Requirement already satisfied: async-timeout>=4.0.3 in /opt/conda/lib/python3.11/site-packages (from asyncpg->-r requirements.txt (line 8)) (4.0.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file->-r requirements.txt (line 1)) (2.5)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r requirements.txt (line 3)) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r requirements.txt (line 3)) (2023.10.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r requirements.txt (line 3)) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r requirements.txt (line 3)) (4.66.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r requirements.txt (line 3)) (6.0.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r requirements.txt (line 3)) (23.1)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r requirements.txt (line 3)) (3.9.2)\n",
      "Requirement already satisfied: pydantic<3.0,>1.1 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r requirements.txt (line 3)) (1.10.12)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2>=2.11.3->llama-cpp-python->-r requirements.txt (line 5)) (2.1.3)\n",
      "Requirement already satisfied: dataclasses-json in /opt/conda/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file->-r requirements.txt (line 1)) (0.6.4)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /opt/conda/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file->-r requirements.txt (line 1)) (1.2.14)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /opt/conda/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file->-r requirements.txt (line 1)) (1.0.8)\n",
      "Requirement already satisfied: httpx in /opt/conda/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file->-r requirements.txt (line 1)) (0.27.0)\n",
      "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.13 in /opt/conda/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file->-r requirements.txt (line 1)) (0.1.13)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /opt/conda/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file->-r requirements.txt (line 1)) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /opt/conda/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file->-r requirements.txt (line 1)) (3.1)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /opt/conda/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file->-r requirements.txt (line 1)) (3.8.1)\n",
      "Requirement already satisfied: openai>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file->-r requirements.txt (line 1)) (1.13.3)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file->-r requirements.txt (line 1)) (2.1.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /opt/conda/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file->-r requirements.txt (line 1)) (10.2.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /opt/conda/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file->-r requirements.txt (line 1)) (8.2.2)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /opt/conda/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file->-r requirements.txt (line 1)) (0.6.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /opt/conda/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file->-r requirements.txt (line 1)) (0.9.0)\n",
      "Requirement already satisfied: PyMuPDFb==1.23.22 in /opt/conda/lib/python3.11/site-packages (from pymupdf<2.0.0,>=1.23.21->llama-index-readers-file->-r requirements.txt (line 1)) (1.23.22)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface->-r requirements.txt (line 3)) (1.12)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface->-r requirements.txt (line 3)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface->-r requirements.txt (line 3)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface->-r requirements.txt (line 3)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface->-r requirements.txt (line 3)) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface->-r requirements.txt (line 3)) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface->-r requirements.txt (line 3)) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface->-r requirements.txt (line 3)) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface->-r requirements.txt (line 3)) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface->-r requirements.txt (line 3)) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /opt/conda/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface->-r requirements.txt (line 3)) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface->-r requirements.txt (line 3)) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /opt/conda/lib/python3.11/site-packages (from torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface->-r requirements.txt (line 3)) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface->-r requirements.txt (line 3)) (12.4.99)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.37.0->llama-index-embeddings-huggingface->-r requirements.txt (line 3)) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.37.0->llama-index-embeddings-huggingface->-r requirements.txt (line 3)) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers<5.0.0,>=4.37.0->llama-index-embeddings-huggingface->-r requirements.txt (line 3)) (0.4.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r requirements.txt (line 3)) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r requirements.txt (line 3)) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r requirements.txt (line 3)) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r requirements.txt (line 3)) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r requirements.txt (line 3)) (1.9.3)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.11/site-packages (from deprecated>=1.2.9.3->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file->-r requirements.txt (line 1)) (1.14.1)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file->-r requirements.txt (line 1)) (4.2.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file->-r requirements.txt (line 1)) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file->-r requirements.txt (line 1)) (1.0.4)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file->-r requirements.txt (line 1)) (3.4)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file->-r requirements.txt (line 1)) (0.14.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file->-r requirements.txt (line 1)) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.11/site-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file->-r requirements.txt (line 1)) (1.3.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file->-r requirements.txt (line 1)) (1.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r requirements.txt (line 3)) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.19.0->huggingface-hub[inference]>=0.19.0->llama-index-embeddings-huggingface->-r requirements.txt (line 3)) (1.26.18)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file->-r requirements.txt (line 1)) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.11/site-packages (from dataclasses-json->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file->-r requirements.txt (line 1)) (3.21.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file->-r requirements.txt (line 1)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file->-r requirements.txt (line 1)) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch<3.0.0,>=2.1.2->llama-index-embeddings-huggingface->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.1->llama-index-readers-file->-r requirements.txt (line 1)) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from huggingface_hub import hf_hub_download\n",
    "#hf_hub_download(repo_id='TheBloke/Mistral-7B-v0.1-GGUF', filename='mistral-7b-v0.1.Q4_K_M.gguf', local_dir='./models', local_dir_use_symlinks=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from models/mistral-7b-v0.1.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4165.37 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3900\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   487.50 MiB\n",
      "llama_new_context_with_model: KV self size  =  487.50 MiB, K (f16):  243.75 MiB, V (f16):  243.75 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =    16.65 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   275.75 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'mistralai_mistral-7b-v0.1', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.llama_cpp import LlamaCPP\n",
    "\n",
    "# model_url = \"https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q4_0.gguf\"\n",
    "model_url = \"https://huggingface.co/TheBloke/Mistral-7B-v0.1-GGUF/resolve/main/mistral-7b-v0.1.Q4_K_M.gguf\"\n",
    "\n",
    "llm = LlamaCPP(\n",
    "    # You can pass in the URL to a GGML model to download it automatically\n",
    "    model_url=None,\n",
    "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "    model_path='models/mistral-7b-v0.1.Q4_K_M.gguf',\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=256,\n",
    "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
    "    context_window=3900,\n",
    "    # kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    # kwargs to pass to __init__()\n",
    "    # set to at least 1 to use GPU\n",
    "    model_kwargs={\"n_gpu_layers\": 0},\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "\n",
    "# db_name = \"vector_db\"\n",
    "# host = \"localhost\"\n",
    "# password = \"postgres\"\n",
    "# port = \"5432\"\n",
    "# user = \"postgres\"\n",
    "# # conn = psycopg2.connect(connection_string)\n",
    "# conn = psycopg2.connect(\n",
    "#     dbname=postgres,\n",
    "#     host=host,\n",
    "#     password=password,\n",
    "#     port=port,\n",
    "#     user=user,\n",
    "# )\n",
    "# conn.autocommit = True\n",
    "\n",
    "# with conn.cursor() as c:\n",
    "#     c.execute(f\"DROP DATABASE IF EXISTS {db_name}\")\n",
    "#     c.execute(f\"CREATE DATABASE {db_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import make_url\n",
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "\n",
    "db_name = \"vectordb\"\n",
    "host = \"db\"\n",
    "password = \"testpwd\"\n",
    "port = \"5432\"\n",
    "user = \"testuser\"\n",
    "vector_store = PGVectorStore.from_params(\n",
    "    database=db_name,\n",
    "    host=host,\n",
    "    password=password,\n",
    "    port=port,\n",
    "    user=user,\n",
    "    table_name=\"llama2_paper\",\n",
    "    embed_dim=384,  # openai embedding dimension\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from llama_index.readers.file import PyMuPDFReader\n",
    "\n",
    "loader = PyMuPDFReader()\n",
    "documents = loader.load(file_path=\"./data/llama2.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "text_parser = SentenceSplitter(\n",
    "    chunk_size=1024,\n",
    "    # separator=\" \",\n",
    ")\n",
    "\n",
    "text_chunks = []\n",
    "# maintain relationship with source doc index, to help inject doc metadata in (3)\n",
    "doc_idxs = []\n",
    "for doc_idx, doc in enumerate(documents):\n",
    "    cur_text_chunks = text_parser.split_text(doc.text)\n",
    "    text_chunks.extend(cur_text_chunks)\n",
    "    doc_idxs.extend([doc_idx] * len(cur_text_chunks))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import TextNode\n",
    "\n",
    "nodes = []\n",
    "for idx, text_chunk in enumerate(text_chunks):\n",
    "    node = TextNode(\n",
    "        text=text_chunk,\n",
    "    )\n",
    "    src_doc = documents[doc_idxs[idx]]\n",
    "    node.metadata = src_doc.metadata\n",
    "    nodes.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes:\n\u001b[0;32m----> 2\u001b[0m     node_embedding \u001b[38;5;241m=\u001b[39m embed_model\u001b[38;5;241m.\u001b[39mget_text_embedding(\n\u001b[1;32m      3\u001b[0m         node\u001b[38;5;241m.\u001b[39mget_content(metadata_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m     )\n\u001b[1;32m      5\u001b[0m     node\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m node_embedding\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/llama_index/core/base/embeddings/base.py:206\u001b[0m, in \u001b[0;36mBaseEmbedding.get_text_embedding\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;124;03mEmbed the input text.\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03mpredefined instructions can be found in embeddings/huggingface_utils.py.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_manager\u001b[38;5;241m.\u001b[39mevent(\n\u001b[1;32m    204\u001b[0m     CBEventType\u001b[38;5;241m.\u001b[39mEMBEDDING, payload\u001b[38;5;241m=\u001b[39m{EventPayload\u001b[38;5;241m.\u001b[39mSERIALIZED: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dict()}\n\u001b[1;32m    205\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m event:\n\u001b[0;32m--> 206\u001b[0m     text_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_text_embedding(text)\n\u001b[1;32m    208\u001b[0m     event\u001b[38;5;241m.\u001b[39mon_end(\n\u001b[1;32m    209\u001b[0m         payload\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m    210\u001b[0m             EventPayload\u001b[38;5;241m.\u001b[39mCHUNKS: [text],\n\u001b[1;32m    211\u001b[0m             EventPayload\u001b[38;5;241m.\u001b[39mEMBEDDINGS: [text_embedding],\n\u001b[1;32m    212\u001b[0m         }\n\u001b[1;32m    213\u001b[0m     )\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m text_embedding\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/llama_index/embeddings/huggingface/base.py:197\u001b[0m, in \u001b[0;36mHuggingFaceEmbedding._get_text_embedding\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get text embedding.\"\"\"\u001b[39;00m\n\u001b[1;32m    196\u001b[0m text \u001b[38;5;241m=\u001b[39m format_text(text, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_instruction)\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embed([text])[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/llama_index/embeddings/huggingface/base.py:161\u001b[0m, in \u001b[0;36mHuggingFaceEmbedding._embed\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# move tokenizer inputs to device\u001b[39;00m\n\u001b[1;32m    157\u001b[0m encoded_input \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    158\u001b[0m     key: val\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_device) \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m encoded_input\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    159\u001b[0m }\n\u001b[0;32m--> 161\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mencoded_input)\n\u001b[1;32m    163\u001b[0m context_layer: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m model_output[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooling \u001b[38;5;241m==\u001b[39m Pooling\u001b[38;5;241m.\u001b[39mCLS:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1013\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1006\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1007\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1008\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1011\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1012\u001b[0m )\n\u001b[0;32m-> 1013\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m   1014\u001b[0m     embedding_output,\n\u001b[1;32m   1015\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[1;32m   1016\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m   1017\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m   1018\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[1;32m   1019\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1020\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1021\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1022\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1023\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1024\u001b[0m )\n\u001b[1;32m   1025\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1026\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    596\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    597\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    598\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m         output_attentions,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[1;32m    608\u001b[0m         hidden_states,\n\u001b[1;32m    609\u001b[0m         attention_mask,\n\u001b[1;32m    610\u001b[0m         layer_head_mask,\n\u001b[1;32m    611\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    612\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    613\u001b[0m         past_key_value,\n\u001b[1;32m    614\u001b[0m         output_attentions,\n\u001b[1;32m    615\u001b[0m     )\n\u001b[1;32m    617\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    487\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\n\u001b[1;32m    498\u001b[0m         hidden_states,\n\u001b[1;32m    499\u001b[0m         attention_mask,\n\u001b[1;32m    500\u001b[0m         head_mask,\n\u001b[1;32m    501\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    502\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mself_attn_past_key_value,\n\u001b[1;32m    503\u001b[0m     )\n\u001b[1;32m    504\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    506\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    419\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    426\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 427\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[1;32m    428\u001b[0m         hidden_states,\n\u001b[1;32m    429\u001b[0m         attention_mask,\n\u001b[1;32m    430\u001b[0m         head_mask,\n\u001b[1;32m    431\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    432\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    433\u001b[0m         past_key_value,\n\u001b[1;32m    434\u001b[0m         output_attentions,\n\u001b[1;32m    435\u001b[0m     )\n\u001b[1;32m    436\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    437\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:308\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    306\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([past_key_value[\u001b[38;5;241m1\u001b[39m], value_layer], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 308\u001b[0m     key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey(hidden_states))\n\u001b[1;32m    309\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue(hidden_states))\n\u001b[1;32m    311\u001b[0m query_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for node in nodes:\n",
    "    node_embedding = embed_model.get_text_embedding(\n",
    "        node.get_content(metadata_mode=\"all\")\n",
    "    )\n",
    "    node.embedding = node_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cb2850a9-c555-4029-a0ea-d51dbb13a605',\n",
       " '96db18ac-b9ee-46c2-bb35-d363caed0378',\n",
       " 'fc3960c1-b83b-4227-9aff-0998e047d4ae',\n",
       " '430298dc-0a40-4088-b5c3-7143d0ebf23c',\n",
       " '3e1b6d4a-0f21-4e2a-ba84-b26638fe0139',\n",
       " '05b17df6-0912-4bad-a5d0-afa4e2192be5',\n",
       " 'e17cc6f5-8567-4ad9-899e-a276f9798ffb',\n",
       " 'ea2126ac-0051-45ee-b016-250b163393ab',\n",
       " '357f1da0-c754-42c6-8d3d-3c2a76e7cf47',\n",
       " '5b3a432c-2f58-482b-8f1b-7c4412aa23ae',\n",
       " 'fd5a42cf-5b21-40a5-bd4e-590724cb2652',\n",
       " 'c54da034-851a-470a-be72-54159528008c',\n",
       " '0d36b98c-53e0-427f-9eb7-b28df3ad7178',\n",
       " '826ab7fc-2e0a-4656-87cd-5bfc9e6b0121',\n",
       " 'eb9e8d30-a33a-4e2b-8fce-5100a74c1c76',\n",
       " '685bc9e0-3858-47f3-8b11-c9be2e425cb3',\n",
       " '1a0fe26a-b514-45bf-990e-4013402b9166',\n",
       " '45955a50-b0c7-485a-afe1-32f73992b291',\n",
       " '224b62e0-8f9f-4c1b-9564-36de01884bee',\n",
       " '77ece127-4ff3-41b1-a292-ad27d5552965',\n",
       " '88d126bc-c5a3-42ff-80ee-4a1491914f4f',\n",
       " '338357a4-7414-456b-8245-aa1eca61f849',\n",
       " '16db3e7a-bd54-4bff-a429-28c8be4e130b',\n",
       " '68d52a6c-acd4-449a-a325-938b13e4783b',\n",
       " '7fb91cbd-96c9-4ad0-823c-bfeadb6f9744',\n",
       " 'd676fb58-eab1-42cd-9c03-dd5661063840',\n",
       " '941b16d3-62ea-40ee-8bdd-f870ec031067',\n",
       " 'a89ffaa8-bcc4-4975-af60-fbf1bc082676',\n",
       " '6c90dd73-cc5b-4e55-b647-efad95644f96',\n",
       " '313b7945-c166-4f07-838c-1ddd77b0b02c',\n",
       " '1c828b2a-186d-4089-93f7-71e18b398efa',\n",
       " '60d76090-e6d8-4aa2-8acc-3e15e7215baf',\n",
       " '0162a08f-e714-4384-81f1-623a75bedcb0',\n",
       " '3565ccef-7776-493e-9db2-ada783a3a98f',\n",
       " '36488d81-62e2-4740-8dbe-403103b5dfce',\n",
       " 'b96a7b32-fa32-43bc-854f-c9157cc56af4',\n",
       " '179a1e87-1138-4808-ae12-07d7466c5cbe',\n",
       " '0d6afb94-c909-438f-8b3c-c1815a159610',\n",
       " '46e4480d-fabe-455a-846c-b40846cb65e5',\n",
       " 'e19a3782-4494-4ab1-a696-7bd682b5a760',\n",
       " '3d921dd3-af96-46d0-9ff4-4515b6a1ed76',\n",
       " '0b1733e1-276c-4232-b5bb-4fba32f2b84e',\n",
       " '43c6608c-604f-47a3-983b-bfc1e7b19575',\n",
       " '727e195f-c907-4be2-b932-cf63e01a69c8',\n",
       " '11c79cec-cccb-4dcc-a2ed-632f641aaccd',\n",
       " '85d70364-899f-407c-b07c-105827adac84',\n",
       " 'dd9442ae-99d4-41a3-99f2-7808407044a2',\n",
       " 'd29aa18a-9dab-4a3f-a4c6-e8c7832da184',\n",
       " '33b4bb2a-0ffb-4668-8ecd-adddcc3d0945',\n",
       " 'e72f0bea-b31f-43dd-806d-8fcfa7bea060',\n",
       " '26d0b062-4628-4479-bd59-989d3080f2ac',\n",
       " 'fc4e1631-6158-4e0b-8848-94d61f1bd9b8',\n",
       " 'fffd1457-7138-46e0-b9bb-49b4621629ec',\n",
       " 'e697d0fa-a594-4af0-9aeb-596410720ac5',\n",
       " '8ae5dd3e-93c7-4dda-b2f6-11cf84c61367',\n",
       " '4bcff03e-f29b-43e4-b2e5-458ed8c91463',\n",
       " 'd5d8bca0-591a-4d20-83ab-93e838de11db',\n",
       " '70e5d1cf-8535-4ec5-8579-a396fd41f803',\n",
       " 'cfc0797b-5b33-4b91-82ca-89c39ae9dd18',\n",
       " 'a72c4255-3c50-4dde-b3a5-ebdb7dba20d0',\n",
       " '29c7686f-f1fb-4261-9b10-309069a38ab1',\n",
       " '5368d9d2-ffb9-4bfb-8446-8aaa342a7ae9',\n",
       " '2e950fec-a669-46e7-a7f4-3a843925f1d7',\n",
       " '0d7881f4-dd18-4c4f-a52a-fc55d193422f',\n",
       " '88b7c043-d482-475c-8ee0-12b550507672',\n",
       " 'e844c73e-2a15-492c-a79b-71482b528f2f',\n",
       " '23b96c20-8a5b-4945-a590-e028ece56f6f',\n",
       " '8e86bbcc-7d91-4548-84de-39f69cf8d1bc',\n",
       " '2fe39cdb-5e92-4d75-8d53-e51ad26fa68e',\n",
       " 'fe6ce5b4-313f-4690-ad87-3f832332d915',\n",
       " 'e34d982d-b964-4a70-ad5c-2dde51e1a5bc',\n",
       " '28b96aee-4070-4e62-b423-3db9612a4649',\n",
       " '10432e34-0feb-4ca0-855d-30a2c7040c01',\n",
       " '70a5ad25-5f7d-432c-a70d-f7126080eb28',\n",
       " '04df51c0-f79c-4ea0-a4ed-37949f54a399',\n",
       " '990c6e13-d491-4b8f-9072-ee1be4093e72',\n",
       " '8ff07144-da38-4fe3-898a-5c63564c5fa8',\n",
       " 'bf1e72f0-17cb-4aef-8dfa-8e6c5ad99c7f',\n",
       " '8ae16093-6f9f-4bb5-bcec-db04534cbc55',\n",
       " 'ad52c02b-a176-48c7-8243-15f894a6170c',\n",
       " '3c4dd515-f5e8-449f-9925-982b941a7a81',\n",
       " '46560c07-2c55-4819-a946-daedc25a5c71',\n",
       " '998d0bc7-effd-4ceb-9b31-fb8f90d8c3a9',\n",
       " 'd7f720ee-fbd1-40e9-be72-7d2028e08743',\n",
       " 'be8cabfa-2a4b-4b0b-a36c-d1cbdaac702a',\n",
       " '8f60ba84-bfe2-40e2-9fbb-8e631d149723',\n",
       " 'dcdeef9c-bbb0-4163-b76a-725f8dcb8bbb',\n",
       " 'dd9ff0b4-0366-48fa-b07b-0ee14befc32a',\n",
       " '551e043a-572a-4729-b774-b32595b22f19',\n",
       " '7156b037-c6b5-4f2d-a5c6-73fd682025a4',\n",
       " 'cb645f69-6437-42e8-950c-635923e5d9f5',\n",
       " '8cc03ef7-33fe-45b8-ae6a-342406b2e84c',\n",
       " 'eb618451-0d21-4b08-8943-9b51bbf2a702',\n",
       " 'ad02884a-47fc-4236-9800-a756b0e2bd30',\n",
       " 'a03b39c4-9224-4e56-94f7-a6b547349b9c',\n",
       " '045d112a-1384-4b57-a6e7-5c2c99c06ae9',\n",
       " '6bc6be9b-5991-4131-94e9-d1d6ab24835c',\n",
       " '66498388-75d0-493e-accc-8ded81c74243',\n",
       " '37cadaab-9962-4c90-a628-2a0a4a7de95f',\n",
       " 'f17c2a6e-5a22-4103-a2e3-f2b97c98a375',\n",
       " '807d6e08-46c9-4bd2-b700-91144a6480c4',\n",
       " 'f6083dd1-0c9c-47be-8048-19a7142e2bcf',\n",
       " '9cc6b826-8082-4433-81aa-0659308ed5c1',\n",
       " 'cd8075bb-e330-49b4-b125-42327178615d',\n",
       " '1da71a49-2a36-46e2-9f23-e75341bce8d7',\n",
       " '5555f9f9-6dba-483d-85a2-301d15ec7544',\n",
       " 'fa20c4cf-ee71-4748-8861-161b93c9bb35']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.add(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = \"Can you tell me about the key concepts for safety finetuning\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = embed_model.get_query_embedding(query_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.vector_stores import VectorStoreQuery\n",
    "\n",
    "query_mode = \"default\"\n",
    "# query_mode = \"sparse\"\n",
    "# query_mode = \"hybrid\"\n",
    "\n",
    "vector_store_query = VectorStoreQuery(\n",
    "    query_embedding=query_embedding, similarity_top_k=2, mode=query_mode\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TruthfulQA \n",
      "ToxiGen \n",
      "MPT\n",
      "7B\n",
      "29.13\n",
      "22.32\n",
      "30B\n",
      "35.25\n",
      "22.61\n",
      "Falcon\n",
      "7B\n",
      "25.95\n",
      "14.53\n",
      "40B\n",
      "40.39\n",
      "23.44\n",
      "Llama 1\n",
      "7B\n",
      "27.42\n",
      "23.00\n",
      "13B\n",
      "41.74\n",
      "23.08\n",
      "33B\n",
      "44.19\n",
      "22.57\n",
      "65B\n",
      "48.71\n",
      "21.77\n",
      "Llama 2\n",
      "7B\n",
      "33.29\n",
      "21.25\n",
      "13B\n",
      "41.86\n",
      "26.10\n",
      "34B\n",
      "43.45\n",
      "21.19\n",
      "70B\n",
      "50.18\n",
      "24.60\n",
      "Table 11: Evaluation of pretrained LLMs on automatic safety benchmarks. For TruthfulQA, we present the\n",
      "percentage of generations that are both truthful and informative (the higher the better). For ToxiGen, we\n",
      "present the percentage of toxic generations (the smaller, the better).\n",
      "Benchmarks give a summary view of model capabilities and behaviors that allow us to understand general\n",
      "patterns in the model, but they do not provide a fully comprehensive view of the impact the model may have\n",
      "on people or real-world outcomes; that would require study of end-to-end product deployments. Further\n",
      "testing and mitigation should be done to understand bias and other social issues for the specific context\n",
      "in which a system may be deployed. For this, it may be necessary to test beyond the groups available in\n",
      "the BOLD dataset (race, religion, and gender). As LLMs are integrated and deployed, we look forward to\n",
      "continuing research that will amplify their potential for positive impact on these important social issues.\n",
      "4.2\n",
      "Safety Fine-Tuning\n",
      "In this section, we describe our approach to safety fine-tuning, including safety categories, annotation\n",
      "guidelines, and the techniques we use to mitigate safety risks. We employ a process similar to the general\n",
      "fine-tuning methods as described in Section 3, with some notable differences related to safety concerns.\n",
      "Specifically, we use the following techniques in safety fine-tuning:\n",
      "1. Supervised Safety Fine-Tuning: We initialize by gathering adversarial prompts and safe demonstra-\n",
      "tions that are then included in the general supervised fine-tuning process (Section 3.1). This teaches\n",
      "the model to align with our safety guidelines even before RLHF, and thus lays the foundation for\n",
      "high-quality human preference data annotation.\n",
      "2. Safety RLHF: Subsequently, we integrate safety in the general RLHF pipeline described in Sec-\n",
      "tion 3.2.2. This includes training a safety-specific reward model and gathering more challenging\n",
      "adversarial prompts for rejection sampling style fine-tuning and PPO optimization.\n",
      "3. Safety Context Distillation: Finally, we refine our RLHF pipeline with context distillation (Askell\n",
      "et al., 2021b). This involves generating safer model responses by prefixing a prompt with a safety\n",
      "preprompt, e.g., You are a safe and responsible assistant, and then fine-tuning the model on the safer\n",
      "responses without the preprompt, which essentially distills the safety preprompt (context) into the\n",
      "model. We use a targeted approach that allows our safety reward model to choose whether to use\n",
      "context distillation for each sample.\n",
      "4.2.1\n",
      "Safety Categories and Annotation Guidelines\n",
      "Based on limitations of LLMs known from prior work, we design instructions for our annotation team to\n",
      "create adversarial prompts along two dimensions: a risk category, or potential topic about which the LLM\n",
      "could produce unsafe content; and an attack vector, or question style to cover different varieties of prompts\n",
      "that could elicit bad model behaviors.\n",
      "The risk categories considered can be broadly divided into the following three categories: illicit and criminal\n",
      "activities (e.g., terrorism, theft, human trafficking); hateful and harmful activities (e.g., defamation, self-\n",
      "harm, eating disorders, discrimination); and unqualified advice (e.g., medical advice, financial advice, legal\n",
      "23\n"
     ]
    }
   ],
   "source": [
    "query_result = vector_store.query(vector_store_query)\n",
    "print(query_result.nodes[0].get_content())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.schema import NodeWithScore\n",
    "from typing import Optional\n",
    "\n",
    "nodes_with_scores = []\n",
    "for index, node in enumerate(query_result.nodes):\n",
    "    score: Optional[float] = None\n",
    "    if query_result.similarities is not None:\n",
    "        score = query_result.similarities[index]\n",
    "    nodes_with_scores.append(NodeWithScore(node=node, score=score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import QueryBundle\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from typing import Any, List\n",
    "\n",
    "\n",
    "class VectorDBRetriever(BaseRetriever):\n",
    "    \"\"\"Retriever over a postgres vector store.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vector_store: PGVectorStore,\n",
    "        embed_model: Any,\n",
    "        query_mode: str = \"default\",\n",
    "        similarity_top_k: int = 2,\n",
    "    ) -> None:\n",
    "        \"\"\"Init params.\"\"\"\n",
    "        self._vector_store = vector_store\n",
    "        self._embed_model = embed_model\n",
    "        self._query_mode = query_mode\n",
    "        self._similarity_top_k = similarity_top_k\n",
    "        super().__init__()\n",
    "\n",
    "    def _retrieve(self, query_bundle: QueryBundle) -> List[NodeWithScore]:\n",
    "        \"\"\"Retrieve.\"\"\"\n",
    "        query_embedding = embed_model.get_query_embedding(\n",
    "            query_bundle.query_str\n",
    "        )\n",
    "        vector_store_query = VectorStoreQuery(\n",
    "            query_embedding=query_embedding,\n",
    "            similarity_top_k=self._similarity_top_k,\n",
    "            mode=self._query_mode,\n",
    "        )\n",
    "        query_result = vector_store.query(vector_store_query)\n",
    "\n",
    "        nodes_with_scores = []\n",
    "        for index, node in enumerate(query_result.nodes):\n",
    "            score: Optional[float] = None\n",
    "            if query_result.similarities is not None:\n",
    "                score = query_result.similarities[index]\n",
    "            nodes_with_scores.append(NodeWithScore(node=node, score=score))\n",
    "\n",
    "        return nodes_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = VectorDBRetriever(\n",
    "    vector_store, embed_model, query_mode=\"default\", similarity_top_k=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "query_engine = RetrieverQueryEngine.from_args(retriever, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   40041.65 ms\n",
      "llama_print_timings:      sample time =      65.91 ms /   143 runs   (    0.46 ms per token,  2169.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =   61818.87 ms /   764 tokens (   80.91 ms per token,    12.36 tokens per second)\n",
      "llama_print_timings:        eval time =   30660.24 ms /   142 runs   (  215.92 ms per token,     4.63 tokens per second)\n",
      "llama_print_timings:       total time =   92929.78 ms /   906 tokens\n"
     ]
    }
   ],
   "source": [
    "query_str = \"How does Llama 2 perform compared to other open-source models?\"\n",
    "\n",
    "response = query_engine.query(query_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the context information, Llama 2 performs on par or better than other open-source models such as PaLM (540B) on almost all benchmarks, but there is still a significant gap in performance between Llama 2 and closed-source models such as GPT-3.5 and GPT-4.\n"
     ]
    }
   ],
   "source": [
    "print(str(response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additionally, Llama 2 70B model outperforms all open-source models.\n",
      "In addition to open-source models, we also compare Llama 2 70B results to closed-source models. As shown\n",
      "in Table 4, Llama 2 70B is close to GPT-3.5 (OpenAI, 2023) on MMLU and GSM8K, but there is a significant\n",
      "gap on coding benchmarks. Llama 2 70B results are on par or better than PaLM (540B) (Chowdhery et al.,\n",
      "2022) on almost all benchmarks. There is still a large gap in performance between Llama 2 70B and GPT-4\n",
      "and PaLM-2-L.\n",
      "We also analysed the potential data contamination and share the details in Section A.6.\n",
      "Benchmark (shots)\n",
      "GPT-3.5\n",
      "GPT-4\n",
      "PaLM\n",
      "PaLM-2-L\n",
      "Llama 2\n",
      "MMLU (5-shot)\n",
      "70.0\n",
      "86.4\n",
      "69.3\n",
      "78.3\n",
      "68.9\n",
      "TriviaQA (1-shot)\n",
      "\n",
      "\n",
      "81.4\n",
      "86.1\n",
      "85.0\n",
      "Natural Questions (1-shot)\n",
      "\n",
      "\n",
      "29.3\n",
      "37.5\n",
      "33.0\n",
      "GSM8K (8-shot)\n",
      "57.1\n",
      "92.0\n",
      "56.5\n",
      "80.7\n",
      "56.8\n",
      "HumanEval (0-shot)\n",
      "48.1\n",
      "67.0\n",
      "26.2\n",
      "\n",
      "29.9\n",
      "BIG-Bench Hard (3-shot)\n",
      "\n",
      "\n",
      "52.3\n",
      "65.7\n",
      "51.2\n",
      "Table 4: Comparison to closed-source models on academic benchmarks. Results for GPT-3.5 and GPT-4\n",
      "are from OpenAI (2023). Results for the PaLM model are from Chowdhery et al. (2022). Results for the\n",
      "PaLM-2-L are from Anil et al. (2023).\n",
      "3\n",
      "Fine-tuning\n",
      "Llama 2-Chat is the result of several months of research and iterative applications of alignment techniques,\n",
      "including both instruction tuning and RLHF, requiring significant computational and annotation resources.\n",
      "In this section, we report on our experiments and findings using supervised fine-tuning (Section 3.1), as\n",
      "well as initial and iterative reward modeling (Section 3.2.2) and RLHF (Section 3.2.3). We also share a\n",
      "new technique, Ghost Attention (GAtt), which we find helps control dialogue flow over multiple turns\n",
      "(Section 3.3). See Section 4.2 for safety evaluations on fine-tuned models.\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "print(response.source_nodes[0].get_content())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   98013.41 ms\n",
      "llama_print_timings:      sample time =       8.26 ms /    15 runs   (    0.55 ms per token,  1816.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =  198249.40 ms /  1018 tokens (  194.74 ms per token,     5.13 tokens per second)\n",
      "llama_print_timings:        eval time =    7279.57 ms /    14 runs   (  519.97 ms per token,     1.92 tokens per second)\n",
      "llama_print_timings:       total time =  205590.94 ms /  1032 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A.7\n",
      "Model Card\n",
      "Table 52 presents a model card (Mitchell et al., 2018; Anil et al., 2023) that summarizes details of the models.\n",
      "Model Details\n",
      "Model Developers\n",
      "Meta AI\n",
      "Variations\n",
      "Llama 2 comes in a range of parameter sizes7B, 13B, and 70Bas well as\n",
      "pretrained and fine-tuned variations.\n",
      "Input\n",
      "Models input text only.\n",
      "Output\n",
      "Models generate text only.\n",
      "Model Architecture\n",
      "Llama 2 is an auto-regressive language model that uses an optimized transformer\n",
      "architecture. The tuned versions use supervised fine-tuning (SFT) and reinforce-\n",
      "ment learning with human feedback (RLHF) to align to human preferences for\n",
      "helpfulness and safety.\n",
      "Model Dates\n",
      "Llama 2 was trained between January 2023 and July 2023.\n",
      "Status\n",
      "This is a static model trained on an offline dataset. Future versions of the tuned\n",
      "models will be released as we improve model safety with community feedback.\n",
      "License\n",
      "A custom commercial license is available at:\n",
      "ai.meta.com/resources/\n",
      "models-and-libraries/llama-downloads/\n",
      "Where to send com-\n",
      "ments\n",
      "Instructions on how to provide feedback or comments on the model can be\n",
      "found in the model README, or by opening an issue in the GitHub repository\n",
      "(https://github.com/facebookresearch/llama/).\n",
      "Intended Use\n",
      "Intended Use Cases\n",
      "Llama 2 is intended for commercial and research use in English. Tuned models\n",
      "are intended for assistant-like chat, whereas pretrained models can be adapted\n",
      "for a variety of natural language generation tasks.\n",
      "Out-of-Scope Uses\n",
      "Use in any manner that violates applicable laws or regulations (including trade\n",
      "compliance laws). Use in languages other than English. Use in any other way\n",
      "that is prohibited by the Acceptable Use Policy and Licensing Agreement for\n",
      "Llama 2.\n",
      "Hardware and Software (Section 2.2)\n",
      "Training Factors\n",
      "We used custom training libraries, Metas Research Super Cluster, and produc-\n",
      "tion clusters for pretraining. Fine-tuning, annotation, and evaluation were also\n",
      "performed on third-party cloud compute.\n",
      "Carbon Footprint\n",
      "Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware\n",
      "of type A100-80GB (TDP of 350-400W). Estimated total emissions were 539\n",
      "tCO2eq, 100% of which were offset by Metas sustainability program.\n",
      "Training Data (Sections 2.1 and 3)\n",
      "Overview\n",
      "Llama 2 was pretrained on 2 trillion tokens of data from publicly available\n",
      "sources. The fine-tuning data includes publicly available instruction datasets, as\n",
      "well as over one million new human-annotated examples. Neither the pretraining\n",
      "nor the fine-tuning datasets include Meta user data.\n",
      "Data Freshness\n",
      "The pretraining data has a cutoff of September 2022, but some tuning data is\n",
      "more recent, up to July 2023.\n",
      "Evaluation Results\n",
      "See evaluations for pretraining (Section 2); fine-tuning (Section 3); and safety (Section 4).\n",
      "Ethical Considerations and Limitations (Section 5.2)\n",
      "Llama 2 is a new technology that carries risks with use. Testing conducted to date has been in\n",
      "English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs,\n",
      "Llama 2s potential outputs cannot be predicted in advance, and the model may in some instances\n",
      "produce inaccurate or objectionable responses to user prompts. Therefore, before deploying any\n",
      "applications of Llama 2, developers should perform safety testing and tuning tailored to their\n",
      "specific applications of the model. Please see the Responsible Use Guide available available at\n",
      "https://ai.meta.com/llama/responsible-user-guide\n",
      "Table 52: Model card for Llama 2.\n",
      "77\n"
     ]
    }
   ],
   "source": [
    "query_str = \"What is the parameter count of a Llama 2 model?\"\n",
    "response = query_engine.query(query_str)\n",
    "print(response.source_nodes[0].get_content())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
